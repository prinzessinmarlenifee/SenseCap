{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prinzessinmarlenifee/SenseCap/blob/main/SenseCap_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrIa2PP6Wcfv"
      },
      "source": [
        "# Imu Cnn Training Pipeline für 3 Sensoren pro Session (head, wrist, seat)\n",
        "# Leave-One-Session-Out Validierung (18 Sessions) mit CNN+LSTM bei 60 Hz Samplingrate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hinweise zur Ausführung:\n",
        "\n",
        "    Alle Abhängigkeiten installieren (Zelle 3) – anschließend brauchst du das nur einmal pro Notebook.\n",
        "\n",
        "    Drive mounten (Zelle 2) immer als erstes ausführen, um Zugriff auf /content/drive/... zu haben.\n",
        "\n",
        "    Pfade anpassen:\n",
        "\n",
        "        Stelle sicher, dass base_dir = '/content/drive/MyDrive/IMU_Sessions/' korrekt ist.\n",
        "\n",
        "        In jedem Unterordner (SessionXX) müssen genau vier Dateien liegen:\n",
        "\n",
        "            hot.json\n",
        "\n",
        "            head.csv\n",
        "\n",
        "            wrist.csv\n",
        "\n",
        "            seat.csv\n",
        "\n",
        "    Code-Zellen einzeln ausführen (Zelle 5 kann komplett auf einmal ausgeführt werden).\n",
        "    → Nach kurzer Zeit (abhängig von GPU) siehst du pro Session die Anzahl der Fenster und die Test-Accuracy.\n",
        "\n",
        "5. Was passiert Schritt für Schritt im Code?\n",
        "\n",
        "    session_dirs = sorted([...])\n",
        "    Liest die 18 Unterordner ein.\n",
        "\n",
        "    parse_hot_labels(...)\n",
        "    Wandelt die Zeitstempel in hot.json in ein frame_labels-Array um (Länge = Anzahl der Zeilen in head.csv).\n",
        "\n",
        "    window_data_multiple_sensors(...)\n",
        "    Erzeugt für jeden 60-Frame-Abschnitt (1 s bei 60 Hz) je Sensor ein Array mit 6 Features, kombiniert drei Sensoren zu 18 Features und weist ein dominantes Label zu (ignoriert „Unknown“).\n",
        "\n",
        "    Daten sammeln\n",
        "    Für jede Session werden X_windows (Fenster) und y_windows (Labels) in Listen sessions_X und sessions_y gespeichert.\n",
        "\n",
        "    Leave-One-Session-Out-Schleife\n",
        "\n",
        "        Eine Session wird als Testset unbeachtet behalten.\n",
        "\n",
        "        Alle anderen Sessions werden zu X_train, y_train zusammengefügt.\n",
        "\n",
        "        Auf y_train wird ein LabelEncoder() gefittet; dieselbe Kodierung wird auf y_test angewandt.\n",
        "\n",
        "    Modell-Definition\n",
        "\n",
        "        Zwei Conv1D-Schichten (jeweils 64 Filter, Kernelgröße 3)\n",
        "\n",
        "        MaxPooling1D(pool_size=2) → halbiert die Zeitschritt-Dimension\n",
        "\n",
        "        Dropout(0.3) → verhindert Overfitting\n",
        "\n",
        "        LSTM(64) → lernt zeitliche Abhängigkeiten\n",
        "\n",
        "        Dense(100) + Dense(n_classes) → endgültige Klassifikation\n",
        "\n",
        "    Training\n",
        "\n",
        "        epochs=20, batch_size=32, validation_split=0.1\n",
        "\n",
        "        Modell sieht 20-mal alle Trainingsfenster (aufgeteilt in Batches à 32)\n",
        "\n",
        "    Evaluation\n",
        "\n",
        "        model.evaluate(X_test, y_test_enc) liefert Test-Accuracy.\n",
        "\n",
        "        classification_report(...) zeigt Precision/Recall/F1 pro Klasse.\n",
        "\n",
        "        ConfusionMatrixDisplay plottet die Matrix für die jeweilige Test-Session.\n",
        "\n",
        "    Zusammenfassung\n",
        "\n",
        "        Durchschnittliche Genauigkeit über alle 18 Sessions\n",
        "\n",
        "        Export als session_accuracy_summary.csv"
      ],
      "metadata": {
        "id": "cjCLCQUPCmIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYB0_lcKHyzC",
        "outputId": "a775bc6a-6ea8-4773-ff82-8394e5dfc220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Verzeichnis gefunden. Enthält folgende Ordner/Sessions:\n",
            "['Session_01', 'Session_02', 'Session_03', 'Session_04', 'Session_05', 'Session_06', 'Session_07', 'Session_08', 'Session_09', 'Session_10'] …\n"
          ]
        }
      ],
      "source": [
        "# --- PARAMETER ---\n",
        "sampling_rate = 60       # 60 Hz nach SDI-Algorithmus\n",
        "window_size = 60         # 1 Sekunde = 60 Frames\n",
        "step_size = 30           # 50% Überlappung\n",
        "\n",
        "# prompt: mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Basisverzeichnis, in dem alle Session-Ordner liegen\n",
        "base_dir = '/content/drive/MyDrive/ML-MTB-Modell/IMU-Sessions/'  #Drive-Pfad\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Prüfe, ob das Verzeichnis existiert und zeige die ersten Sessions an\n",
        "if os.path.exists(base_dir):\n",
        "    print(\"Verzeichnis gefunden. Enthält folgende Ordner/Sessions:\")\n",
        "    print(sorted(os.listdir(base_dir))[:10], \"…\")\n",
        "else:\n",
        "    print(\"Achtung: Pfad existiert nicht, bitte base_dir anpassen!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK6FHg4RWbOt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Io9CmYDORjY",
        "outputId": "4ac3e7b8-b67d-4598-81f1-2259904f329a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " championship_data.npy\t   input_video.mp4     ptbdb_abnormal.csv\n",
            "'Colab Notebooks'\t   Jump_Data\t       ptbdb_normal.csv\n",
            " Einstelldatenblatt.gdoc   ML-MTB-Modell      'Unbenanntes Formular.gform'\n",
            " Einstelldatenblatt.pdf    picture_ski_2.jpg\n",
            " hurensohnbild.jpg\t   picture_ski.jpg\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#google-Drive inhalt anzeigen für Überblick/Überbrüfen\n",
        "!ls /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "Pa4WrZ-DInRa",
        "outputId": "0719b1cf-0612-4cdd-82ba-998a7fa09784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gefundene Sessions: 14 -> ['Session_01', 'Session_02', 'Session_03', 'Session_04', 'Session_05', 'Session_06', 'Session_07', 'Session_08', 'Session_09', 'Session_10', 'Session_11', 'Session_12', 'Session_13', 'Session_14']\n",
            "\n",
            "Lade Session: Session_01\n",
            "⏱ Synch angewendet: Head=1386, Wrist=1236, Seat=1035\n",
            " → 1951 Fenster, 3 Klassen in Session Session_01\n",
            "\n",
            "Anzahl gültiger Sessions nach Filterung: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_idx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-205380b1678e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Train-Daten: alle anderen Sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msessions_X\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msessions_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-205380b1678e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Train-Daten: alle anderen Sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msessions_X\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msessions_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_idx' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "layers = tf.keras.layers\n",
        "models = tf.keras.models\n",
        "\n",
        "\n",
        "# --- PARAMETER ---\n",
        "sampling_rate = 60       # 60 Hz nach SDI-Algorithmus\n",
        "window_size = 60         # 1 Sekunde = 60 Frames\n",
        "step_size = 30           # 50% Überlappung\n",
        "\n",
        "# Basisverzeichnis (sollte die 18 Session-Ordner enthalten)\n",
        "base_dir = '/content/drive/MyDrive/ML-MTB-Modell/IMU-Sessions/'\n",
        "\n",
        "\n",
        "# Session-Ordner erkennen\n",
        "session_dirs = sorted([\n",
        "    d for d in os.listdir(base_dir)\n",
        "    if os.path.isdir(os.path.join(base_dir, d))\n",
        "])\n",
        "print(f\"Gefundene Sessions: {len(session_dirs)} -> {session_dirs}\")\n",
        "\n",
        "# Funktion: Hot-Labels pro Frame generieren\n",
        "def parse_hot_labels(json_path, total_frames):\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    button_press_str = data['button_presses']\n",
        "    entries = button_press_str.strip().split(';')\n",
        "\n",
        "    label_changes = []\n",
        "    for entry in entries:\n",
        "        if ':' in entry:\n",
        "            label, frame = entry.strip().split(':')\n",
        "            label = label.strip()\n",
        "            # Falsche Schreibweise korrigieren\n",
        "            if label.lower() == 'peadling':\n",
        "                label = 'Pedaling'\n",
        "            label_changes.append((int(frame.strip()), label))\n",
        "\n",
        "    # Standardmäßig 'Unknown' setzen\n",
        "    frame_labels = ['Unknown'] * total_frames\n",
        "    for i, (start_frame, label) in enumerate(label_changes):\n",
        "        end_frame = label_changes[i + 1][0] if i + 1 < len(label_changes) else total_frames\n",
        "        for f in range(start_frame, min(end_frame, total_frames)):\n",
        "            frame_labels[f] = label\n",
        "    return frame_labels\n",
        "\n",
        "# Funktion: Fensterung & Label-Zuweisung für 3 Sensoren zusammengeführt\n",
        "def window_data_multiple_sensors(head_data, wrist_data, seat_data, frame_labels):\n",
        "    X_windows, y_windows = [], []\n",
        "    total_frames = len(frame_labels)\n",
        "\n",
        "    # Annahme: head_data, wrist_data, seat_data haben alle dieselbe Anzahl Zeilen\n",
        "    for start in range(0, total_frames - window_size + 1, step_size):\n",
        "        end = start + window_size\n",
        "        # Fenster je Sensor: (window_size, 6)\n",
        "        win_h = head_data[start:end]\n",
        "        win_w = wrist_data[start:end]\n",
        "        win_s = seat_data[start:end]\n",
        "        # Kombiniere: (window_size, 18) – 3 Sensoren • 6 Achsen\n",
        "        window = np.concatenate([win_h, win_w, win_s], axis=1)\n",
        "\n",
        "        # Label-Fenster\n",
        "        label_window = frame_labels[start:end]\n",
        "        label_counts = Counter(label_window)\n",
        "        dominant_label = label_counts.most_common(1)[0][0]\n",
        "        if dominant_label == 'Unknown':\n",
        "            continue\n",
        "\n",
        "        X_windows.append(window)\n",
        "        y_windows.append(dominant_label)\n",
        "\n",
        "    return np.array(X_windows), np.array(y_windows)\n",
        "\n",
        "# --- DATEN SAMMELN: alle Sessions laden, Fenster und Labels erzeugen ---\n",
        "sessions_X = []\n",
        "sessions_y = []\n",
        "\n",
        "for sess_dir in session_dirs:\n",
        "    print(f\"\\nLade Session: {sess_dir}\")\n",
        "    session_path = os.path.join(base_dir, sess_dir)\n",
        "\n",
        "    # 1) Sensor-Dateien anhand ihres Prefix finden\n",
        "    def find_sensor_file(folder, prefix):\n",
        "        for f in os.listdir(folder):\n",
        "            if f.lower().startswith(prefix.lower()):\n",
        "                return os.path.join(folder, f)\n",
        "        raise FileNotFoundError(f\"❌ Keine Datei mit Prefix '{prefix}' in {folder} gefunden.\")\n",
        "\n",
        "    head_path  = find_sensor_file(session_path, 'Head_')\n",
        "    wrist_path = find_sensor_file(session_path, 'Wrist_')\n",
        "    seat_path  = find_sensor_file(session_path, 'Seat_')\n",
        "\n",
        "    # 2) IMU-Daten laden: Kopf, Handgelenk, Sitz (je 6 Spalten)\n",
        "    head_data  = pd.read_csv(head_path).values   # numpy array, shape (n_frames, 6)\n",
        "    wrist_data = pd.read_csv(wrist_path).values\n",
        "    seat_data  = pd.read_csv(seat_path).values\n",
        "\n",
        "    total_frames   = head_data.shape[0]\n",
        "\n",
        "\n",
        "    # Funktion: finde Hot-Datei mit Suffix \"_hot.json\"\n",
        "    def find_hot_file(folder):\n",
        "        for f in os.listdir(folder):\n",
        "            if f.lower().endswith('_hot.json'):\n",
        "                return os.path.join(folder, f)\n",
        "        raise FileNotFoundError(f\"❌ Keine Datei mit Suffix '_hot.json' in {folder} gefunden.\")\n",
        "    # Labels aus hot.json erstellen\n",
        "    hot_path = find_hot_file(session_path)\n",
        "\n",
        "    frame_labels   = parse_hot_labels(hot_path, total_frames)\n",
        "\n",
        "    # Beispiel: Synch-Datei einlesen (wenn vorhanden)\n",
        "    synch_path = os.path.join(session_path, 'synch_data.json')\n",
        "    if os.path.exists(synch_path):\n",
        "        with open(synch_path, 'r') as f:\n",
        "            synch = json.load(f)\n",
        "        offset_head  = int(synch.get(\"Head Sensor Video (Timestamp) in Frames\", 0))\n",
        "        offset_wrist = int(synch.get(\"Wrist Sensor Video (Timestamp) in Frames\", 0))\n",
        "        offset_seat  = int(synch.get(\"Seat Sensor Video (Timestamp) in Frames\", 0))\n",
        "\n",
        "        head_data  = head_data[offset_head:]\n",
        "        wrist_data = wrist_data[offset_wrist:]\n",
        "        seat_data  = seat_data[offset_seat:]\n",
        "        print(f\"⏱ Synch angewendet: Head={offset_head}, Wrist={offset_wrist}, Seat={offset_seat}\")\n",
        "    else:\n",
        "        print(f\"ℹ️ Keine synch_data.json gefunden für {sess_dir}\")\n",
        "    # Kürzen auf gemeinsame Länge\n",
        "    min_length = min(len(head_data), len(wrist_data), len(seat_data))\n",
        "    head_data  = head_data[:min_length]\n",
        "    wrist_data = wrist_data[:min_length]\n",
        "    seat_data  = seat_data[:min_length]\n",
        "    frame_labels = frame_labels[:min_length]\n",
        "\n",
        "    # Fensterung & Labeling\n",
        "    X_win, y_win = window_data_multiple_sensors(head_data, wrist_data, seat_data, frame_labels)\n",
        "\n",
        "    sessions_X.append(X_win)\n",
        "    sessions_y.append(y_win)\n",
        "    print(f\" → {len(X_win)} Fenster, {len(np.unique(y_win))} Klassen in Session {sess_dir}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # --- Session-Säuberung vor dem Training ---\n",
        "    sessions_X_cleaned = []\n",
        "    sessions_y_cleaned = []\n",
        "\n",
        "    for x, y in zip(sessions_X, sessions_y):\n",
        "        if x.shape[0] > 0 and len(x.shape) == 3:\n",
        "            sessions_X_cleaned.append(x)\n",
        "            sessions_y_cleaned.append(y)\n",
        "        else:\n",
        "            print(f\"Session mit leerem oder ungültigem Shape übersprungen: {x.shape}\")\n",
        "\n",
        "    # Jetzt mit den bereinigten Sessions weiterarbeiten:\n",
        "    sessions_X = sessions_X_cleaned\n",
        "    sessions_y = sessions_y_cleaned\n",
        "\n",
        "    print(f\"Anzahl gültiger Sessions nach Filterung: {len(sessions_X)}\")\n",
        "\n",
        "\n",
        "    # Train-Daten: alle anderen Sessions\n",
        "    X_train = np.concatenate([x for i, x in enumerate(sessions_X) if i != test_idx])\n",
        "    y_train = np.concatenate([y for i, y in enumerate(sessions_y) if i != test_idx])\n",
        "\n",
        "    # Label Encoding (gemeinsam auf Trainingsdaten fitten)\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(y_train)\n",
        "    y_test_enc = le.transform(y_test)\n",
        "\n",
        "    # Modell-Architektur (CNN + LSTM)\n",
        "    model = models.Sequential([\n",
        "        layers.Conv1D(64, 3, activation='relu', input_shape=X_train.shape[1:]),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.LSTM(64),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(len(le.classes_), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Training (mit 10% Validierungssplit aus Trainingsdaten)\n",
        "    history = model.fit(\n",
        "        X_train, y_train_enc,\n",
        "        validation_split=0.1,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluation auf Test-Session\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test_enc, verbose=0)\n",
        "    print(f\"✅ Test-Accuracy für {sess_name}: {test_acc:.2f}\")\n",
        "    all_accuracies.append(test_acc)\n",
        "    accuracy_summary['Session'].append(sess_name)\n",
        "    accuracy_summary['Accuracy'].append(test_acc)\n",
        "\n",
        "    # Klassifikationsbericht\n",
        "    y_pred_probs = model.predict(X_test, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    print(\"\\nKlassifikationsbericht:\")\n",
        "    print(classification_report(y_test_enc, y_pred_classes, target_names=le.classes_))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test_enc, y_pred_classes)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
        "    disp.plot(xticks_rotation=45)\n",
        "    plt.title(f\"Confusion Matrix – {sess_name}\")\n",
        "    plt.show()\n",
        "\n",
        "# --- Zusammenfassung über alle Sessions ---\n",
        "mean_acc = np.mean(all_accuracies)\n",
        "print(\"\\n📈 Gesamtergebnis: Leave-One-Session-Out\")\n",
        "for sess, acc in zip(accuracy_summary['Session'], accuracy_summary['Accuracy']):\n",
        "    print(f\"  {sess}: {acc:.2f}\")\n",
        "print(f\"\\n✅ Durchschnittliche Genauigkeit über alle Sessions: {mean_acc:.2f}\\n\")\n",
        "\n",
        "# Optional: Speichere Ergebnis als CSV\n",
        "df_summary = pd.DataFrame(accuracy_summary)\n",
        "output_csv = 'session_accuracy_summary.csv'\n",
        "df_summary.to_csv(output_csv, index=False)\n",
        "print(f\"📁 Zusammenfassung gespeichert in: {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWuEkPWFBAvw",
        "outputId": "ac6fb4c6-3fca-4e63-d0b6-2fa14d5fca39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session 0: shape = (1951, 60, 6)\n",
            "Session 1: shape = (1546, 60, 6)\n",
            "Session 2: shape = (1379, 60, 6)\n",
            "Session 3: shape = (1836, 60, 6)\n",
            "Session 4: shape = (1822, 60, 6)\n",
            "Session 5: shape = (2079, 60, 6)\n",
            "Session 6: shape = (1965, 60, 6)\n",
            "Session 7: shape = (0,)\n",
            "Session 8: shape = (1948, 60, 6)\n",
            "Session 9: shape = (2037, 60, 6)\n",
            "Session 10: shape = (2157, 60, 6)\n",
            "Session 11: shape = (1946, 60, 35)\n",
            "Session 12: shape = (2144, 60, 35)\n",
            "Session 13: shape = (1810, 60, 35)\n"
          ]
        }
      ],
      "source": [
        "#print shape for debug\n",
        "\n",
        "\n",
        "  #ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 6 has 1 dimension(s)\n",
        "for i, x in enumerate(sessions_X):\n",
        "    print(f\"Session {i}: shape = {np.array(x).shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMRluOq_LlmK",
        "outputId": "7ef5926b-c218-49f9-dae3-cbf81ba18199"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({6: 1})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "shapes = [x.shape for x in sessions_X]\n",
        "feature_counts = [shape[2] if len(shape) == 3 else None for shape in shapes]\n",
        "print(Counter(feature_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "tV7rXj0ZBmNg",
        "outputId": "1f80d5e7-ab95-449b-8814-2fd65f8d8fea"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_synced' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-eb667476c507>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_synced\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msessions_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_synced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_synced' is not defined"
          ]
        }
      ],
      "source": [
        "if len(x_synced) > 0:\n",
        "    sessions_X.append(x_synced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erA50imoM8BD"
      },
      "source": [
        "5. Was passiert Schritt für Schritt im Code?\n",
        "\n",
        "    session_dirs = sorted([...])\n",
        "    Liest die 18 Unterordner ein.\n",
        "\n",
        "    parse_hot_labels(...)\n",
        "    Wandelt die Zeitstempel in hot.json in ein frame_labels-Array um (Länge = Anzahl der Zeilen in head.csv).\n",
        "\n",
        "    window_data_multiple_sensors(...)\n",
        "    Erzeugt für jeden 60-Frame-Abschnitt (1 s bei 60 Hz) je Sensor ein Array mit 6 Features, kombiniert drei Sensoren zu 18 Features und weist ein dominantes Label zu (ignoriert „Unknown“).\n",
        "\n",
        "    Daten sammeln\n",
        "    Für jede Session werden X_windows (Fenster) und y_windows (Labels) in Listen sessions_X und sessions_y gespeichert.\n",
        "\n",
        "    Leave-One-Session-Out-Schleife\n",
        "\n",
        "        Eine Session wird als Testset unbeachtet behalten.\n",
        "\n",
        "        Alle anderen Sessions werden zu X_train, y_train zusammengefügt.\n",
        "\n",
        "        Auf y_train wird ein LabelEncoder() gefittet; dieselbe Kodierung wird auf y_test angewandt.\n",
        "\n",
        "    Modell-Definition\n",
        "\n",
        "        Zwei Conv1D-Schichten (jeweils 64 Filter, Kernelgröße 3)\n",
        "\n",
        "        MaxPooling1D(pool_size=2) → halbiert die Zeitschritt-Dimension\n",
        "\n",
        "        Dropout(0.3) → verhindert Overfitting\n",
        "\n",
        "        LSTM(64) → lernt zeitliche Abhängigkeiten\n",
        "\n",
        "        Dense(100) + Dense(n_classes) → endgültige Klassifikation\n",
        "\n",
        "    Training\n",
        "\n",
        "        epochs=20, batch_size=32, validation_split=0.1\n",
        "\n",
        "        Modell sieht 20-mal alle Trainingsfenster (aufgeteilt in Batches à 32)\n",
        "\n",
        "    Evaluation\n",
        "\n",
        "        model.evaluate(X_test, y_test_enc) liefert Test-Accuracy.\n",
        "\n",
        "        classification_report(...) zeigt Precision/Recall/F1 pro Klasse.\n",
        "\n",
        "        ConfusionMatrixDisplay plottet die Matrix für die jeweilige Test-Session.\n",
        "\n",
        "    Zusammenfassung\n",
        "\n",
        "        Durchschnittliche Genauigkeit über alle 18 Sessions\n",
        "\n",
        "        Export als session_accuracy_summary.csv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPROIo1VqXWpdzgB/F9zos6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}