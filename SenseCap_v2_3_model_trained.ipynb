{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJZv7IznA7VAn3lpaRBjLa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prinzessinmarlenifee/SenseCap/blob/main/SenseCap_v2_3_model_trained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjgPMqQLDD-q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Versuch 2: Fused Excel sheets ohne sync\n",
        "\n"
      ],
      "metadata": {
        "id": "hKNNiSrdDEqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk6qFLbhDJlr",
        "outputId": "030940cb-42b7-464e-ab51-06a6b35d2846"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "layers = tf.keras.layers\n",
        "models = tf.keras.models\n"
      ],
      "metadata": {
        "id": "OtTD-qyzDXoH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PARAMETER ---\n",
        "sampling_rate = 60       # 60 Hz nach SDI-Algorithmus\n",
        "window_size = 60         # 1 Sekunde = 60 Frames\n",
        "step_size = 30           # 50% Überlappung\n",
        "\n",
        "# Basisverzeichnis (sollte die 18 Session-Ordner enthalten)\n",
        "base_dir = '/content/drive/MyDrive/ML-MTB-Modell/IMU-Sessions/'\n"
      ],
      "metadata": {
        "id": "8d9oN1zpDqKo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sessions einlesen"
      ],
      "metadata": {
        "id": "bcvlHHFOD36z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session_dirs = sorted([\n",
        "    d for d in os.listdir(base_dir)\n",
        "    if os.path.isdir(os.path.join(base_dir, d))\n",
        "])\n",
        "print(f\"Gefundene Sessions: {len(session_dirs)} -> {session_dirs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqMHmK7wD2YX",
        "outputId": "8044537a-92a5-492b-e15a-5bea462d9239"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gefundene Sessions: 13 -> ['Session_01', 'Session_02', 'Session_03', 'Session_04', 'Session_05', 'Session_06', 'Session_07', 'Session_09', 'Session_10', 'Session_11', 'Session_12', 'Session_13', 'Session_14']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fuktionen zum LAbel-Parsing und Datei finden"
      ],
      "metadata": {
        "id": "8puFV7gaEAMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 parse_hot_labels: Liest die _hot.json-Datei ein, erstellt für jeden Frame ein Label\n",
        "def parse_hot_labels(json_path, total_frames):\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    entries = data['button_presses'].strip().split(';')\n",
        "\n",
        "    label_changes = []\n",
        "    for entry in entries:\n",
        "        if ':' in entry:\n",
        "            label, frame = entry.strip().split(':')\n",
        "            label = label.strip()\n",
        "            # Korrigiere evtl. \"Peadling\" → \"Pedaling\"\n",
        "            if label.lower() == 'peadling':\n",
        "                label = 'Pedaling'\n",
        "            label_changes.append((int(frame.strip()), label))\n",
        "\n",
        "    frame_labels = ['Unknown'] * total_frames\n",
        "    for i, (start_frame, label) in enumerate(label_changes):\n",
        "        end_frame = label_changes[i + 1][0] if i + 1 < len(label_changes) else total_frames\n",
        "        for f in range(start_frame, min(end_frame, total_frames)):\n",
        "            frame_labels[f] = label\n",
        "    return frame_labels\n",
        "\n",
        "# 5.2 find_sensor_file: Findet CSV-Datei, deren Name mit dem Prefix beginnt (Head_, Wrist_, Seat_)\n",
        "def find_sensor_file(folder, prefix):\n",
        "    for f in os.listdir(folder):\n",
        "        if f.lower().startswith(prefix.lower()):\n",
        "            return os.path.join(folder, f)\n",
        "    raise FileNotFoundError(f\"❌ Keine Datei mit Prefix '{prefix}' in {folder} gefunden.\")\n",
        "\n",
        "# 5.3 find_hot_file: Findet JSON-Datei, deren Name auf \"_hot.json\" endet\n",
        "def find_hot_file(folder):\n",
        "    for f in os.listdir(folder):\n",
        "        if f.lower().endswith('_hot.json'):\n",
        "            return os.path.join(folder, f)\n",
        "    raise FileNotFoundError(f\"❌ Keine Datei mit Suffix '_hot.json' in {folder} gefunden.\")\n"
      ],
      "metadata": {
        "id": "lo3wDSUeD-TT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Fensterung für drei Sensoren kombinieren"
      ],
      "metadata": {
        "id": "FYZhfgCTESe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def window_data_multiple_sensors(head_data, wrist_data, seat_data, frame_labels):\n",
        "    X_windows, y_windows = [], []\n",
        "    total_frames = len(frame_labels)\n",
        "\n",
        "    # Annahme: head_data, wrist_data, seat_data haben alle dieselbe Anzahl Zeilen = total_frames\n",
        "    for start in range(0, total_frames - window_size + 1, step_size):\n",
        "        end = start + window_size\n",
        "\n",
        "        win_h = head_data[start:end]    # (window_size, 6)\n",
        "        win_w = wrist_data[start:end]\n",
        "        win_s = seat_data[start:end]\n",
        "        window = np.concatenate([win_h, win_w, win_s], axis=1)  # → (window_size, 18)\n",
        "\n",
        "        label_window = frame_labels[start:end]\n",
        "        dominant_label = Counter(label_window).most_common(1)[0][0]\n",
        "        if dominant_label == 'Unknown':\n",
        "            continue\n",
        "\n",
        "        X_windows.append(window)\n",
        "        y_windows.append(dominant_label)\n",
        "\n",
        "    return np.array(X_windows), np.array(y_windows)\n"
      ],
      "metadata": {
        "id": "_RKQP4OGEKi5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#debug\n",
        "print(f\"🔍 Session {sess_dir} → Fenster: {len(X_win)}, Shape: {X_win.shape if len(X_win) > 0 else 'n/a'}\")\n"
      ],
      "metadata": {
        "id": "zBEtLxfeeqLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_csv_headers(path, label):\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    print(f\"\\n📋 {label} → Datei: {os.path.basename(path)}\")\n",
        "    print(\"   Zeile 0:\", lines[0].strip())\n",
        "    print(\"   Zeile 1:\", lines[1].strip())\n",
        "\n",
        "print_csv_headers(head_path, 'Head')\n",
        "print_csv_headers(wrist_path, 'Wrist')\n",
        "print_csv_headers(seat_path, 'Seat')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "9aYhfR4i7aA3",
        "outputId": "948a88aa-71bb-4022-ce06-c5bee861ca64"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'head_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-944b91b6d29e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   Zeile 1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint_csv_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Head'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint_csv_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrist_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Wrist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint_csv_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseat_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Seat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'head_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Daten einlesen und Fenster / Labels erzeugen\n",
        "→ Nach Ausführung siehst du für jede Session etwa: “→ 153 Fenster, 3 Klassen” etc."
      ],
      "metadata": {
        "id": "yjBHXCZBEaWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vorbereitung\n",
        "sessions_X = []\n",
        "sessions_y = []\n",
        "valid_sessions = []  # <- neue Liste! mit nur valid sessions\n",
        "skipped_sessions = []\n",
        "\n",
        "#features definieren:\n",
        "features = ['Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z']\n",
        "\n",
        "#Funktionen für Daten einlesen\n",
        "def smart_feature_filter(df):\n",
        "    # alles lowercase und leerzeichenfrei vergleichen\n",
        "    keep = [col for col in df.columns if any(kw in col.lower() for kw in ['euler', 'acc', 'gyr'])]\n",
        "    return df[keep]\n",
        "\n",
        "def print_csv_headers(path, label):\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    print(f\"\\n📋 {label} → Datei: {os.path.basename(path)}\")\n",
        "    print(\"   Zeile 0:\", lines[0].strip())\n",
        "    print(\"   Zeile 1:\", lines[1].strip())\n",
        "\n",
        "\n",
        "def inspect_sensor_csv(path):\n",
        "    df = pd.read_csv(path, sep=',', skiprows=1)\n",
        "    df.columns = df.columns.str.strip()\n",
        "    print(f\"📊 {os.path.basename(path)}: {df.shape[1]} Spalten\")\n",
        "    print(\"   → Spaltennamen:\", df.columns.tolist())\n",
        "\n",
        "\n",
        "#def load_sensor_csv(path, expected_columns=None):\n",
        " # df = pd.read_csv(path, sep=',', skiprows=1)\n",
        "  #df.columns = df.columns.str.strip()\n",
        "  #if expected_columns:\n",
        "   #   df = df[expected_columns]\n",
        "#  df = df.apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n",
        " # return df.values\n",
        "\n",
        "\n",
        "#def load_sensor_csv(path):\n",
        "#    df = pd.read_csv(path, sep=',', skiprows=1)\n",
        "#    df.columns = df.columns.str.strip()\n",
        "#    df = smart_feature_filter(df)\n",
        "#    df = df.apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n",
        "#    return df.values\n",
        "\n",
        "\n",
        "def load_sensor_csv(path):\n",
        "  import csv\n",
        "\n",
        "  # Erste zwei Zeilen lesen\n",
        "  with open(path, 'r') as f:\n",
        "      reader = csv.reader(f)\n",
        "      first_line = next(reader)\n",
        "      second_line = next(reader)\n",
        "\n",
        "  # Prüfen ob erste Zeile ein Header ist (z. B. mit bekannten Schlüsselwörtern)\n",
        "  first_line_str = \",\".join(first_line).lower()\n",
        "  if any(kw in first_line_str for kw in ['euler', 'acc', 'gyr']):\n",
        "      skip = 0\n",
        "  else:\n",
        "      skip = 1\n",
        "\n",
        "  # Einlesen\n",
        "  df = pd.read_csv(path, sep=',', skiprows=skip)\n",
        "  df.columns = df.columns.str.strip()\n",
        "\n",
        "  # Features filtern\n",
        "  df = smart_feature_filter(df)\n",
        "\n",
        "  # Numerisch umwandeln und NaN behandeln\n",
        "  df = df.apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n",
        "\n",
        "  return df.values\n",
        "\n",
        "  print(f\"🔍 {os.path.basename(path)}: Header {'erste Zeile' if skip==0 else 'zweite Zeile'}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Hauptschleife ---\n",
        "for sess_dir in session_dirs:\n",
        "    print(f\"\\n📂 Lade Session: {sess_dir}\")\n",
        "    session_path = os.path.join(base_dir, sess_dir)\n",
        "\n",
        "\n",
        "    # 7.1 Sensor-Dateien finden\n",
        "    head_path  = find_sensor_file(session_path, 'Head_')\n",
        "    wrist_path = find_sensor_file(session_path, 'Wrist_')\n",
        "    seat_path  = find_sensor_file(session_path, 'Seat_')\n",
        "\n",
        "    # 7.2 Hot-JSON-Datei finden\n",
        "    hot_path = find_hot_file(session_path)\n",
        "\n",
        "    inspect_sensor_csv(head_path)\n",
        "    inspect_sensor_csv(wrist_path)\n",
        "    inspect_sensor_csv(seat_path)\n",
        "\n",
        "\n",
        "    #print csv-heads for debugging & checking (oben definierte function print_csv_headers)\n",
        "    #print_csv_headers(head_path, 'Head')\n",
        "    #print_csv_headers(wrist_path, 'Wrist')\n",
        "    #print_csv_headers(seat_path, 'Seat')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 7.3 IMU-Daten laden\n",
        "    #funktion um imu laden\n",
        "    #aktuelles Problem: header in der zweiten Zeile, seperator ',' , erkennt nur zwei spalten beim einlesen\n",
        "\n",
        "\n",
        "\n",
        "    head_data  = load_sensor_csv(head_path)\n",
        "    wrist_data = load_sensor_csv(wrist_path)\n",
        "    seat_data  = load_sensor_csv(seat_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"📊 Sensorlängen: Head={head_data.shape}, Wrist={wrist_data.shape}, Seat={seat_data.shape}\")\n",
        "\n",
        "    #expected_features = 27  # 3 Sensoren × 9 Features (oben definiert)\n",
        "   # if X_win.shape[1:] != (window_size, expected_features):\n",
        "    #      print(f\"⚠️ Session {sess_dir} hat Format {X_win.shape[1:]}, wird übersprungen.\")\n",
        "     #     skipped_sessions.append(sess_dir)\n",
        "      #    continue\n",
        "\n",
        "\n",
        "\n",
        "    #7.4 Labels laden\n",
        "    total_frames = min(head_data.shape[0], wrist_data.shape[0], seat_data.shape[0])\n",
        "    frame_labels = parse_hot_labels(hot_path, total_frames)\n",
        "\n",
        "\n",
        "\n",
        "    # 7.5 Sicherheitskürzung (später optional mit synch.json ersetzen)\n",
        "    head_data  = head_data[:total_frames]\n",
        "    wrist_data = wrist_data[:total_frames]\n",
        "    seat_data  = seat_data[:total_frames]\n",
        "    frame_labels = frame_labels[:total_frames]\n",
        "\n",
        "\n",
        "    # 7.6 Fensterung & Label-Zuweisung\n",
        "    X_win, y_win = window_data_multiple_sensors(head_data, wrist_data, seat_data, frame_labels)\n",
        "\n",
        "    # 5. Gültigkeit prüfen\n",
        "    if len(X_win) == 0:\n",
        "        print(f\"⚠️  Session {sess_dir} übersprungen – keine gültigen Fenster.\")\n",
        "        skipped_sessions.append(sess_dir)\n",
        "        continue\n",
        "\n",
        "    expected_features = 27  # oder dynamisch aus den Daten\n",
        "\n",
        "    if X_win.shape[1:] != (window_size, expected_features):\n",
        "        print(f\"⚠️  Session {sess_dir} hat Format {X_win.shape[1:]}, wird übersprungen.\")\n",
        "        skipped_sessions.append(sess_dir)\n",
        "        continue\n",
        "\n",
        "\n",
        "        # 6. Speichern\n",
        "    sessions_X.append(X_win)\n",
        "    sessions_y.append(y_win)\n",
        "    valid_sessions.append(sess_dir)\n",
        "    print(f\"✅ Session {sess_dir}: {len(X_win)} Fenster, {len(np.unique(y_win))} Klassen\")\n",
        "\n",
        "# --- Zusammenfassung ---\n",
        "print(\"\\n✅ Verwendete Sessions:\")\n",
        "for idx, sess in enumerate(valid_sessions):\n",
        "    print(f\"  {sess}: {sessions_X[idx].shape}\")\n",
        "\n",
        "if skipped_sessions:\n",
        "    print(\"\\n⛔️ Übersprungene Sessions:\")\n",
        "    for s in skipped_sessions:\n",
        "        print(f\"  {s}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hAmaAMFEKM_",
        "outputId": "7344b0ae-1b71-4ffa-e11c-5a2e2b43a677"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📂 Lade Session: Session_01\n",
            "📊 Head_D422CD00563B_20230713_082527.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Wrist_D422CD004550_20230713_082527.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Seat_D422CD00456D_20230713_082527.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Sensorlängen: Head=(60077, 9), Wrist=(60078, 9), Seat=(60075, 9)\n",
            "✅ Session Session_01: 1997 Fenster, 3 Klassen\n",
            "\n",
            "📂 Lade Session: Session_02\n",
            "📊 Head_D422CD00563B_20230713_085629.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Wrist_D422CD004550_20230713_085629.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Seat_D422CD00456D_20230713_085629.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Sensorlängen: Head=(48792, 9), Wrist=(48792, 9), Seat=(48784, 9)\n",
            "✅ Session Session_02: 1621 Fenster, 3 Klassen\n",
            "\n",
            "📂 Lade Session: Session_03\n",
            "📊 Head_D422CD00563B_20230713_092642.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Wrist_D422CD004550_20230713_092642.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Seat_D422CD00456D_20230713_092642.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Sensorlängen: Head=(42308, 9), Wrist=(42305, 9), Seat=(42307, 9)\n",
            "✅ Session Session_03: 1407 Fenster, 3 Klassen\n",
            "\n",
            "📂 Lade Session: Session_04\n",
            "📊 Head_D422CD00563B_20230717_070932.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Wrist_D422CD004550_20230717_070932.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Seat_D422CD00456D_20230717_070932.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Sensorlängen: Head=(56013, 9), Wrist=(56019, 9), Seat=(56015, 9)\n",
            "✅ Session Session_04: 1864 Fenster, 3 Klassen\n",
            "\n",
            "📂 Lade Session: Session_05\n",
            "📊 Head_D422CD00563B_20230717_070932.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Wrist_D422CD004550_20230717_070932.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Seat_D422CD00456D_20230717_070932.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Sensorlängen: Head=(56013, 9), Wrist=(56019, 9), Seat=(56015, 9)\n",
            "✅ Session Session_05: 1858 Fenster, 4 Klassen\n",
            "\n",
            "📂 Lade Session: Session_06\n",
            "📊 Head_D422CD00563B_20230720_074713.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Wrist_D422CD004550_20230720_074713.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Seat_D422CD00456D_20230720_074713.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Sensorlängen: Head=(64027, 9), Wrist=(64026, 9), Seat=(64025, 9)\n",
            "✅ Session Session_06: 2116 Fenster, 4 Klassen\n",
            "\n",
            "📂 Lade Session: Session_07\n",
            "📊 Head_D422CD00563B_20230720_082728.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Wrist_D422CD004550_20230720_082728.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Seat_D422CD00456D_20230720_082728.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Sensorlängen: Head=(59920, 9), Wrist=(59917, 9), Seat=(59921, 9)\n",
            "✅ Session Session_07: 1991 Fenster, 4 Klassen\n",
            "\n",
            "📂 Lade Session: Session_09\n",
            "📊 Head_D422CD00563B_20230724_072319.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Wrist_D422CD004550_20230724_072319.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Seat_D422CD00456D_20230724_072319.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Sensorlängen: Head=(59464, 9), Wrist=(59465, 9), Seat=(59468, 9)\n",
            "✅ Session Session_09: 1979 Fenster, 4 Klassen\n",
            "\n",
            "📂 Lade Session: Session_10\n",
            "📊 Head_D422CD004576_20230725_070718.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Wrist_D422CD004550_20230725_070718.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Seat_D422CD00456D_20230725_070718.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n",
            "📊 Sensorlängen: Head=(62167, 9), Wrist=(62163, 9), Seat=(62163, 9)\n",
            "✅ Session Session_10: 2061 Fenster, 4 Klassen\n",
            "\n",
            "📂 Lade Session: Session_11\n",
            "📊 Head_D422CD004576_20230727_073528.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-0bd4558f1fa9>:25: DtypeWarning: Columns (5,6,7,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path, sep=',', skiprows=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Wrist_D422CD004550_20230727_073528.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-0bd4558f1fa9>:25: DtypeWarning: Columns (5,6,7,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path, sep=',', skiprows=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Seat_D422CD00456D_20230727_073528.csv: 12 Spalten\n",
            "   → Spaltennamen: ['PacketCounter', 'SampleTimeFine', 'Euler_X', 'Euler_Y', 'Euler_Z', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z', 'Unnamed: 11']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-0bd4558f1fa9>:65: DtypeWarning: Columns (5,6,7,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path, sep=',', skiprows=skip)\n",
            "<ipython-input-12-0bd4558f1fa9>:65: DtypeWarning: Columns (5,6,7,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path, sep=',', skiprows=skip)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Sensorlängen: Head=(65763, 9), Wrist=(65763, 9), Seat=(65763, 9)\n",
            "✅ Session Session_11: 2181 Fenster, 4 Klassen\n",
            "\n",
            "📂 Lade Session: Session_12\n",
            "📊 Head_D422CD004576_20230801_075834.csv: 12 Spalten\n",
            "   → Spaltennamen: ['0', '416010770', '-4.522172451019287', '48.04213333129883', '21.0351619720459', '0', '0.1', '0.2', '0.3', '0.4', '0.5', '']\n",
            "📊 Wrist_D422CD004550_20230801_075834.csv: 12 Spalten\n",
            "   → Spaltennamen: ['0', '532295258', '-42.42717361450195', '45.996009826660156', '-55.192440032958984', '0', '0.1', '0.2', '0.3', '0.4', '0.5', '']\n",
            "📊 Seat_D422CD00456D_20230801_075834.csv: 11 Spalten\n",
            "   → Spaltennamen: ['0', '409881162', '2202328491210930', '-6035955047607420', '-8357672119140620', '0.00000000000000', '0.00000000000000.1', '0.00000000000000.2', '0.00000000000000.3', '0.00000000000000.4', '0.00000000000000.5']\n",
            "📊 Sensorlängen: Head=(59523, 9), Wrist=(59524, 9), Seat=(59514, 9)\n",
            "✅ Session Session_12: 1967 Fenster, 4 Klassen\n",
            "\n",
            "📂 Lade Session: Session_13\n",
            "📊 Head_D422CD004576_20230802_080027.csv: 12 Spalten\n",
            "   → Spaltennamen: ['0', '209493532', '-7.16727876663208', '47.413570404052734', '-87.12100219726562', '0', '0.1', '0.2', '0.3', '0.4', '0.5', '']\n",
            "📊 Wrist_D422CD004550_20230802_080027.csv: 12 Spalten\n",
            "   → Spaltennamen: ['0', '358144070', '-41.73847198486328', '23.458871841430664', '-68.4642105102539', '0', '0.1', '0.2', '0.3', '0.4', '0.5', '']\n",
            "📊 Seat_D422CD00456D_20230802_080027.csv: 11 Spalten\n",
            "   → Spaltennamen: ['0', '435628135', '199753963947296', '-6415881347656250', '-16434582519531200', '0.00000000000000', '0.00000000000000.1', '0.00000000000000.2', '0.00000000000000.3', '0.00000000000000.4', '0.00000000000000.5']\n",
            "📊 Sensorlängen: Head=(65046, 9), Wrist=(66494, 9), Seat=(65037, 9)\n",
            "✅ Session Session_13: 2161 Fenster, 4 Klassen\n",
            "\n",
            "📂 Lade Session: Session_14\n",
            "📊 Head_D422CD004576_20230803_073423.csv: 12 Spalten\n",
            "   → Spaltennamen: ['0', '23311239', '5.965623378753662', '-45.560245513916016', '164.4220428466797', '0', '0.1', '0.2', '0.3', '0.4', '0.5', '']\n",
            "📊 Wrist_D422CD004550_20230803_073423.csv: 12 Spalten\n",
            "   → Spaltennamen: ['0', '177062498', '78.41714477539062', '7.847972869873047', '119.1408920288086', '0', '0.1', '0.2', '0.3', '0.4', '0.5', '']\n",
            "📊 Seat_D422CD00456D_20230803_073423.csv: 11 Spalten\n",
            "   → Spaltennamen: ['0', '310191965', '7921337127685540', '-2267286872863770', '-12943373107910100', '0.00000000000000', '0.00000000000000.1', '0.00000000000000.2', '0.00000000000000.3', '0.00000000000000.4', '0.00000000000000.5']\n",
            "📊 Sensorlängen: Head=(58097, 9), Wrist=(58096, 9), Seat=(58086, 9)\n",
            "✅ Session Session_14: 1869 Fenster, 4 Klassen\n",
            "\n",
            "✅ Verwendete Sessions:\n",
            "  Session_01: (1997, 60, 27)\n",
            "  Session_02: (1621, 60, 27)\n",
            "  Session_03: (1407, 60, 27)\n",
            "  Session_04: (1864, 60, 27)\n",
            "  Session_05: (1858, 60, 27)\n",
            "  Session_06: (2116, 60, 27)\n",
            "  Session_07: (1991, 60, 27)\n",
            "  Session_09: (1979, 60, 27)\n",
            "  Session_10: (2061, 60, 27)\n",
            "  Session_11: (2181, 60, 27)\n",
            "  Session_12: (1967, 60, 27)\n",
            "  Session_13: (2161, 60, 27)\n",
            "  Session_14: (1869, 60, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 Dann ist deine Fensterform:\n",
        "\n",
        "    3 Sensoren × 9 Spalten = 27 Features\n",
        "    → Fenster-Shape: (window_size, 27) = (60, 27)\n",
        "\n",
        "\n",
        "Euler_X, Euler_Y, Euler_Z\n",
        "\n",
        "\n",
        "Acc_X, Acc_Y, Acc_Z\n",
        "\n",
        "Gyr_X, Gyr_Y, Gyr_Z\n",
        "\n",
        "→ = 9 physikalisch sinnvolle Spalten pro Sensor"
      ],
      "metadata": {
        "id": "VEmZyAMMpyKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Debug:shape der daten anzeigen\n",
        "\n",
        "print(\"\\n✅ Preprocessing abgeschlossen. Shape jeder Session:\")\n",
        "for idx, sess in enumerate(session_dirs):\n",
        "    print(f\"{sess}: {sessions_X[idx].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNATwmB6JMDe",
        "outputId": "c1df42fd-b4f6-40b9-d3ee-df895600afb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Preprocessing abgeschlossen. Shape jeder Session:\n",
            "Session_01: (1997, 60, 27)\n",
            "Session_02: (1621, 60, 27)\n",
            "Session_03: (1407, 60, 27)\n",
            "Session_04: (1864, 60, 27)\n",
            "Session_05: (1858, 60, 27)\n",
            "Session_06: (2116, 60, 27)\n",
            "Session_07: (1991, 60, 27)\n",
            "Session_09: (1979, 60, 27)\n",
            "Session_10: (2061, 60, 27)\n",
            "Session_11: (2181, 60, 27)\n",
            "Session_12: (1967, 60, 27)\n",
            "Session_13: (2161, 60, 27)\n",
            "Session_14: (1869, 60, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Leave-one-session-out: Training and Evaluation"
      ],
      "metadata": {
        "id": "8szKmR2qE5RI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔁 Leave-One-Session-Out bedeutet:\n",
        "\n",
        "In jeder Runde trainierst du auf n-1 Sessions\n",
        "und testest auf 1 unbekannte Session – das Modell ist also jedes Mal neu initialisiert.\n",
        "➡️ Das Modell wird nicht besser über die Sessions hinweg, weil es nicht weitertrainiert, sondern immer frisch startet.\n",
        "➡️ Dafür bekommst du aber eine faire Abschätzung, wie gut dein Ansatz auf neuen Nutzern/Sessions funktioniert."
      ],
      "metadata": {
        "id": "YCtpA2zzyzhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📘 Modell-Training mit Checkpoints und Evaluation pro Session\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# 💾 Speicherpfade\n",
        "checkpoint_dir = \"/content/drive/MyDrive/mtb_project/checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "final_model_dir = \"/content/drive/MyDrive/mtb_project/final_models\"\n",
        "os.makedirs(final_model_dir, exist_ok=True)\n",
        "\n",
        "all_accuracies = []\n",
        "accuracy_summary = {'Session': [], 'Accuracy': []}\n",
        "\n",
        "\n",
        "#testet für jede session einzeln\n",
        "for test_idx in range(len(sessions_X)):\n",
        "    sess_name = session_dirs[test_idx]\n",
        "    print(f\"\\n📌 Teste auf Session (unbekannt): {sess_name} ({test_idx+1}/{len(sessions_X)})\")\n",
        "\n",
        "    # Testdaten\n",
        "    X_test = sessions_X[test_idx]\n",
        "    y_test = sessions_y[test_idx]\n",
        "\n",
        "    # Trainingsdaten\n",
        "    X_train = np.concatenate([x for i, x in enumerate(sessions_X) if i != test_idx])\n",
        "    y_train = np.concatenate([y for i, y in enumerate(sessions_y) if i != test_idx])\n",
        "\n",
        "    # Label-Encoding\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(y_train)\n",
        "    y_test_enc = le.transform(y_test)\n",
        "\n",
        "    # Validation Split\n",
        "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "        X_train, y_train_enc, test_size=0.1, random_state=42, stratify=y_train_enc\n",
        "    )\n",
        "\n",
        "    # Modell-Architektur\n",
        "    model = models.Sequential([\n",
        "        layers.Conv1D(64, 3, activation='relu', input_shape=X_train.shape[1:]),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.LSTM(64),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(len(le.classes_), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Checkpoint Callback\n",
        "    checkpoint_cb = ModelCheckpoint(\n",
        "        filepath=os.path.join(checkpoint_dir, f\"{sess_name}_epoch_{{epoch:02d}}.keras\"),\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Training starten\n",
        "    history = model.fit(\n",
        "        X_train_split, y_train_split,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=50,\n",
        "        batch_size=64,\n",
        "        callbacks=[checkpoint_cb]\n",
        "    )\n",
        "\n",
        "    # Finale Modell speichern\n",
        "    model.save(os.path.join(final_model_dir, f\"{sess_name}_final.keras\"))\n",
        "\n",
        "    # Evaluation\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test_enc, verbose=0)\n",
        "    print(f\"✅ Test-Accuracy für {sess_name}: {test_acc:.2f}\")\n",
        "    all_accuracies.append(test_acc)\n",
        "    accuracy_summary['Session'].append(sess_name)\n",
        "    accuracy_summary['Accuracy'].append(test_acc)\n",
        "\n",
        "    # Report & Matrix\n",
        "    y_pred_probs = model.predict(X_test, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "    print(\"\\nKlassifikationsbericht:\")\n",
        "    print(classification_report(y_test_enc, y_pred_classes, target_names=le.classes_))\n",
        "\n",
        "    cm = confusion_matrix(y_test_enc, y_pred_classes)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=le.classes_)\n",
        "    disp.plot(xticks_rotation=45)\n",
        "    plt.title(f\"Confusion Matrix – {sess_name}\")\n",
        "    plt.show()\n",
        "\n",
        "# 🧾 Zusammenfassung speichern\n",
        "summary_df = pd.DataFrame(accuracy_summary)\n",
        "summary_df.to_csv(\"session_accuracy_report.csv\", index=False)\n",
        "print(\"\\n📁 Bericht gespeichert als session_accuracy_report.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy2yUlRWyqic",
        "outputId": "48b7f614-81ef-4fe9-d750-5add7c3474c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📌 Teste auf Session (unbekannt): Session_01 (1/13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5616 - loss: 1.0433\n",
            "Epoch 1: val_loss improved from inf to 0.92286, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_01.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.5617 - loss: 1.0431 - val_accuracy: 0.6101 - val_loss: 0.9229\n",
            "Epoch 2/50\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6042 - loss: 0.9266\n",
            "Epoch 2: val_loss improved from 0.92286 to 0.86363, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_02.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.6042 - loss: 0.9265 - val_accuracy: 0.6495 - val_loss: 0.8636\n",
            "Epoch 3/50\n",
            "\u001b[1m324/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6315 - loss: 0.8689\n",
            "Epoch 3: val_loss improved from 0.86363 to 0.83691, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_03.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6315 - loss: 0.8689 - val_accuracy: 0.6568 - val_loss: 0.8369\n",
            "Epoch 4/50\n",
            "\u001b[1m324/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6307 - loss: 0.8514\n",
            "Epoch 4: val_loss improved from 0.83691 to 0.82035, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_04.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 42ms/step - accuracy: 0.6307 - loss: 0.8513 - val_accuracy: 0.6642 - val_loss: 0.8204\n",
            "Epoch 5/50\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6525 - loss: 0.8175\n",
            "Epoch 5: val_loss improved from 0.82035 to 0.79980, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_05.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 38ms/step - accuracy: 0.6525 - loss: 0.8175 - val_accuracy: 0.6755 - val_loss: 0.7998\n",
            "Epoch 6/50\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6530 - loss: 0.7991\n",
            "Epoch 6: val_loss improved from 0.79980 to 0.78894, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_06.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.6530 - loss: 0.7991 - val_accuracy: 0.6685 - val_loss: 0.7889\n",
            "Epoch 7/50\n",
            "\u001b[1m324/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6530 - loss: 0.7949\n",
            "Epoch 7: val_loss improved from 0.78894 to 0.76860, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_07.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.6530 - loss: 0.7948 - val_accuracy: 0.6824 - val_loss: 0.7686\n",
            "Epoch 8/50\n",
            "\u001b[1m324/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6647 - loss: 0.7822\n",
            "Epoch 8: val_loss improved from 0.76860 to 0.76685, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_08.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.6647 - loss: 0.7822 - val_accuracy: 0.6685 - val_loss: 0.7669\n",
            "Epoch 9/50\n",
            "\u001b[1m324/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6551 - loss: 0.7697\n",
            "Epoch 9: val_loss did not improve from 0.76685\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.6551 - loss: 0.7697 - val_accuracy: 0.6620 - val_loss: 0.7753\n",
            "Epoch 10/50\n",
            "\u001b[1m324/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6680 - loss: 0.7718\n",
            "Epoch 10: val_loss improved from 0.76685 to 0.75136, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_10.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 41ms/step - accuracy: 0.6680 - loss: 0.7718 - val_accuracy: 0.6820 - val_loss: 0.7514\n",
            "Epoch 11/50\n",
            "\u001b[1m324/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6574 - loss: 0.7700\n",
            "Epoch 11: val_loss did not improve from 0.75136\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 42ms/step - accuracy: 0.6574 - loss: 0.7700 - val_accuracy: 0.6759 - val_loss: 0.7613\n",
            "Epoch 12/50\n",
            "\u001b[1m324/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6600 - loss: 0.7552\n",
            "Epoch 12: val_loss improved from 0.75136 to 0.74151, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_12.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 0.6601 - loss: 0.7552 - val_accuracy: 0.6919 - val_loss: 0.7415\n",
            "Epoch 13/50\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6760 - loss: 0.7318\n",
            "Epoch 13: val_loss did not improve from 0.74151\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 38ms/step - accuracy: 0.6760 - loss: 0.7318 - val_accuracy: 0.6755 - val_loss: 0.7469\n",
            "Epoch 14/50\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6692 - loss: 0.7399\n",
            "Epoch 14: val_loss improved from 0.74151 to 0.73298, saving model to /content/drive/MyDrive/mtb_project/checkpoints/Session_01_epoch_14.keras\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.6693 - loss: 0.7399 - val_accuracy: 0.6915 - val_loss: 0.7330\n",
            "Epoch 15/50\n",
            "\u001b[1m324/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6746 - loss: 0.7305\n",
            "Epoch 15: val_loss did not improve from 0.73298\n",
            "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 37ms/step - accuracy: 0.6746 - loss: 0.7305 - val_accuracy: 0.6750 - val_loss: 0.7528\n",
            "Epoch 16/50\n",
            "\u001b[1m285/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.6757 - loss: 0.7339"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🧾 Zusammenfassung Modeltraining\n",
        "##📊 1. Durchschnittliche Accuracy über alle Sessions:"
      ],
      "metadata": {
        "id": "q-yhvJmXzOjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#📊 1. Durchschnittliche Accuracy über alle Sessions:\n",
        "\n",
        "mean_acc = np.mean(all_accuracies)\n",
        "print(\"\\n📈 Zusammenfassung:\")\n",
        "for i, acc in enumerate(all_accuracies):\n",
        "    print(f\"  Session {i+1}: Accuracy = {acc:.2f}\")\n",
        "print(f\"\\n✅ Durchschnittliche Test-Accuracy über alle Sessions: {mean_acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZKwF6fOy75p",
        "outputId": "ca256b36-96a2-471b-bdf0-689eff87a8eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📈 Zusammenfassung:\n",
            "  Session 1: Accuracy = 0.21\n",
            "\n",
            "✅ Durchschnittliche Test-Accuracy über alle Sessions: 0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##📁 2. CSV speichern (falls nicht schon vorhanden):"
      ],
      "metadata": {
        "id": "Evqv6rIczcn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#📁 2. CSV speichern (falls nicht schon vorhanden):\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_summary = pd.DataFrame(accuracy_summary)\n",
        "df_summary['Session'] = [f\"Session_{i+1}\" for i in range(len(all_accuracies))]\n",
        "df_summary.to_csv(\"/content/drive/MyDrive/mtb_project/session_accuracy_report.csv\", index=False)\n",
        "print(\"✅ Bericht gespeichert unter: session_accuracy_report.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AxzlH2XzXA9",
        "outputId": "cc0684b5-4bd9-428b-b823-35d8283e50d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Bericht gespeichert unter: session_accuracy_report.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##🧠 Bonus: Bestes Modell/Session finden"
      ],
      "metadata": {
        "id": "EXhSC-Sk0Vzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bestes model finden\n",
        "best_idx = np.argmax(all_accuracies)\n",
        "print(f\"\\n🏅 Beste Session: {accuracy_summary['Session'][best_idx]} mit Accuracy {all_accuracies[best_idx]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9ySWUfX0WZm",
        "outputId": "4935efc8-600f-4a90-d3f1-ea5c11eedb8f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🏅 Beste Session: Session_01 mit Accuracy 0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_accuracies = []\n",
        "accuracy_summary = {'Session': [], 'Accuracy': []}\n",
        "\n",
        "for test_idx in range(len(sessions_X)):\n",
        "    sess_name = session_dirs[test_idx]\n",
        "    print(f\"\\n📌 Teste auf Session (unbekannt): {sess_name} ({test_idx+1}/{len(sessions_X)})\")\n",
        "\n",
        "    # 8.1 Test-Daten definieren\n",
        "    X_test = sessions_X[test_idx]\n",
        "    y_test = sessions_y[test_idx]\n",
        "\n",
        "\n",
        "\n",
        "    # 8.2 Train-Daten: alle anderen Sessions zusammenschneiden\n",
        "    X_train = np.concatenate([x for i, x in enumerate(sessions_X) if i != test_idx])\n",
        "    y_train = np.concatenate([y for i, y in enumerate(sessions_y) if i != test_idx])\n",
        "\n",
        "    # 8.3 Label-Encoding (fit auf Trainingsdaten, transform auf beides)\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(y_train)\n",
        "    y_test_enc = le.transform(y_test)\n",
        "\n",
        "    # 8.4 Modell-Definition: CNN + LSTM\n",
        "    model = models.Sequential([\n",
        "        layers.Conv1D(64, 3, activation='relu', input_shape=X_train.shape[1:]),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.LSTM(64),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(len(le.classes_), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 8.5 Training (mit 10 % Validierungssplit aus Trainingsdaten)\n",
        "\n",
        "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "    import os\n",
        "\n",
        "    # 1. 📁 Sicherstellen, dass Speicherort existiert\n",
        "    checkpoint_dir = \"/content/drive/MyDrive/mtb_project/checkpoints\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # 2. 🎯 Callback definieren\n",
        "    checkpoint_cb = ModelCheckpoint(\n",
        "        filepath = os.path.join(checkpoint_dir, f\"{sess_name}_epoch_{{epoch:02d}}.keras\"),\n",
        "        save_best_only=False,         # du bekommst jedes Epoch-Modell\n",
        "        save_weights_only=False,      # speichert das ganze Modell, nicht nur Gewichte\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "# 3. 🚀 Training starten (Callback hinzufügen!)\n",
        "\n",
        "\n",
        "    # mit checkpoints zum speichern\n",
        "    history = model.fit(\n",
        "        X_train, y_train_enc,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=50,\n",
        "        batch_size=64,\n",
        "        callbacks=[checkpoint_cb] #-> automatische zwischenspeicherung\n",
        "    )\n",
        "\n",
        "    # 8.6 Evaluation auf Test-Session\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test_enc, verbose=0)\n",
        "    print(f\"✅ Test-Accuracy für {sess_name}: {test_acc:.2f}\")\n",
        "    all_accuracies.append(test_acc)\n",
        "    accuracy_summary['Session'].append(sess_name)\n",
        "    accuracy_summary['Accuracy'].append(test_acc)\n",
        "\n",
        "    # 8.7 Klassifikationsbericht & Confusion Matrix\n",
        "    y_pred_probs = model.predict(X_test, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    print(\"\\nKlassifikationsbericht:\")\n",
        "    print(classification_report(y_test_enc, y_pred_classes, target_names=le.classes_))\n",
        "\n",
        "    cm = confusion_matrix(y_test_enc, y_pred_classes)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=le.classes_)\n",
        "    disp.plot(xticks_rotation=45)\n",
        "    plt.title(f\"Confusion Matrix – {sess_name}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "zlJAElwcE1L8"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧾 Zusammenfassung nach dem Training:\n",
        "\n",
        "Nach der Schleife kannst du am Ende folgendes hinzufügen, um einen Bericht zu erzeugen:\n",
        "\n",
        "📊 Bonus: CSV speichern (optional)"
      ],
      "metadata": {
        "id": "G6M9hNEyTzn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Durchschnittliche Accuracy über alle Sessions\n",
        "mean_acc = np.mean(all_accuracies)\n",
        "print(\"\\n📈 Zusammenfassung:\")\n",
        "for i, acc in enumerate(all_accuracies):\n",
        "    print(f\"  Session {i+1}: Accuracy = {acc:.2f}\")\n",
        "print(f\"\\n✅ Durchschnittliche Test-Accuracy über alle Sessions: {mean_acc:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "#📊 Bonus: CSV speichern (optional)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "summary = {'Session': [], 'Accuracy': []}\n",
        "for i, acc in enumerate(all_accuracies):\n",
        "    summary['Session'].append(f\"Session_{i+1}\")\n",
        "    summary['Accuracy'].append(acc)\n",
        "\n",
        "df_summary = pd.DataFrame(summary)\n",
        "df_summary.to_csv(\"session_accuracy_report.csv\", index=False)\n",
        "print(\"📁 Bericht gespeichert als session_accuracy_report.csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9eFJGGxTutz",
        "outputId": "01130166-2fe5-424a-8cdc-1eb2fb531cdc"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📈 Zusammenfassung:\n",
            "\n",
            "✅ Durchschnittliche Test-Accuracy über alle Sessions: nan\n",
            "📁 Bericht gespeichert als session_accuracy_report.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#💾 II. Modell speichern & später wieder laden (z. B. nach Training)\n",
        "\n",
        "##🔐 Speichern mit TensorFlow/**Keras**"
      ],
      "metadata": {
        "id": "rbaE9UFeMQ8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nach dem Training:\n",
        "#model.save(\"SenseCap_Eventdetection_Model.keras\")  # speichert nur in colab kurzzeitig\n",
        "model.save(\"/content/drive/MyDrive/ML-MTB-Modell/ML-Model_trained/SenseCap_Eventdetection_Model{sess_name}.keras\")\n"
      ],
      "metadata": {
        "id": "9f0wu8rYM2P3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##🔄 Laden\n",
        "\n",
        "Das speichert das gesamte Modell inkl. Architektur, Gewichten und Optimizer-Zustand –exakt da weitermachen, wo man aufgehört hast."
      ],
      "metadata": {
        "id": "Fe6SLk39NNOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/content/drive/MyDrive/ML-MTB-Modell/ML-Model_trained/SenseCap_Eventdetection_Model1.keras\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puTdBnHsNHsC",
        "outputId": "e01895fd-40b2-435b-a7f7-44950f5ebdf9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 24 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_accuracies = []\n",
        "\n",
        "for i in range(len(test_sessions_X)):\n",
        "    X_test = test_sessions_X[i]\n",
        "    y_test = test_sessions_y[i]\n",
        "\n",
        "    _, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    all_accuracies.append(acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "0FK9HYSaUOP0",
        "outputId": "0bb3586c-14d1-4009-e85c-573951307718"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_sessions_X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-49eeb814a429>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sessions_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_sessions_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_sessions_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_sessions_X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Durchschnittliche Accuracy über alle Sessions\n",
        "mean_acc = np.mean(all_accuracies)\n",
        "print(\"\\n📈 Zusammenfassung:\")\n",
        "for i, acc in enumerate(all_accuracies):\n",
        "    print(f\"  Session {i+1}: Accuracy = {acc:.2f}\")\n",
        "print(f\"\\n✅ Durchschnittliche Test-Accuracy über alle Sessions: {mean_acc:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "#📊 Bonus: CSV speichern (optional)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "summary = {'Session': [], 'Accuracy': []}\n",
        "for i, acc in enumerate(all_accuracies):\n",
        "    summary['Session'].append(f\"Session_{i+1}\")\n",
        "    summary['Accuracy'].append(acc)\n",
        "\n",
        "df_summary = pd.DataFrame(summary)\n",
        "df_summary.to_csv(\"session_accuracy_report.csv\", index=False)\n",
        "print(\"📁 Bericht gespeichert als session_accuracy_report.csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cce6369-7927-49d7-a612-51daac80c42b",
        "id": "p3xuxyPOUOsO"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📈 Zusammenfassung:\n",
            "\n",
            "✅ Durchschnittliche Test-Accuracy über alle Sessions: nan\n",
            "📁 Bericht gespeichert als session_accuracy_report.csv\n"
          ]
        }
      ]
    }
  ]
}