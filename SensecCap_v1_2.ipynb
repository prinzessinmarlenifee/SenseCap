{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8P8T76rbwBuYeHTfF4bLz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prinzessinmarlenifee/SenseCap/blob/main/SensecCap_v1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC1fEqJk2X9r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hier sollte nachdem mit einer session getestet wurde, das fÃ¼r alle sessions, mit 1 fused excel sheet fÃ¼r die drei sensoren getestet werden.\n",
        "sprich, hot labels nutzten, und ein excel sheet mit allen drei sensors schon gefused"
      ],
      "metadata": {
        "id": "7BVAhzM92d2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# --- PARAMETER ---\n",
        "sampling_rate = 60  # wie besprochen, evtl step size anpassen\n",
        "window_size = 60\n",
        "step_size = 11\n",
        "\n",
        "# Ordner, in dem deine 18 Sessions liegen\n",
        "base_dir = '/content/drive/MyDrive/IMU_Sessions/'  # ANPASSEN\n",
        "\n",
        "# Alle Session-Ordner suchen (z.â€¯B. 001, 002, ..., 018)\n",
        "session_dirs = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
        "\n",
        "def parse_hot_labels(json_path, total_frames):\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    button_press_str = data['button_presses']\n",
        "    entries = button_press_str.strip().split(';')\n",
        "\n",
        "    label_changes = []\n",
        "    for entry in entries:\n",
        "        if ':' in entry:\n",
        "            label, frame = entry.strip().split(':')\n",
        "            label = label.strip()\n",
        "            if label == 'Peadling':\n",
        "                label = 'Pedaling'  # âœ… Korrektur hier\n",
        "            label_changes.append((int(frame.strip()), label))\n",
        "\n",
        "    frame_labels = ['Unknown'] * total_frames\n",
        "    for i, (start_frame, label) in enumerate(label_changes):\n",
        "        end_frame = label_changes[i + 1][0] if i + 1 < len(label_changes) else total_frames\n",
        "        for f in range(start_frame, min(end_frame, total_frames)):\n",
        "            frame_labels[f] = label\n",
        "    return frame_labels\n",
        "\n",
        "def window_data(imu_data, frame_labels):\n",
        "    X_windows, y_windows = [], []\n",
        "    for start in range(0, len(frame_labels) - window_size + 1, step_size):\n",
        "        end = start + window_size\n",
        "        window = imu_data[start:end]\n",
        "        label_window = frame_labels[start:end]\n",
        "\n",
        "        label_counts = Counter(label_window)\n",
        "        dominant_label = label_counts.most_common(1)[0][0]\n",
        "        if dominant_label == 'Unknown':\n",
        "            continue\n",
        "        X_windows.append(window)\n",
        "        y_windows.append(dominant_label)\n",
        "    return np.array(X_windows), np.array(y_windows)\n",
        "\n",
        "# --- DATEN SAMMELN ---\n",
        "all_X, all_y = [], []\n",
        "sessions_X, sessions_y = [], []\n",
        "\n",
        "for sess_dir in session_dirs:\n",
        "    print(f\"Lade Session {sess_dir}...\")\n",
        "    session_path = os.path.join(base_dir, sess_dir)\n",
        "    hot_path = os.path.join(session_path, 'hot.json')\n",
        "    imu_path = os.path.join(session_path, 'imu.csv')  # â† ANPASSEN je nach Format\n",
        "\n",
        "    # Lade IMU-Daten (angenommen CSV, mit 6 Spalten)\n",
        "    imu_data = np.loadtxt(imu_path, delimiter=',')  # oder pd.read_csv()\n",
        "\n",
        "    total_frames = imu_data.shape[0]\n",
        "    frame_labels = parse_hot_labels(hot_path, total_frames)\n",
        "    X, y = window_data(imu_data, frame_labels)\n",
        "\n",
        "    sessions_X.append(X)\n",
        "    sessions_y.append(y)\n",
        "\n",
        "# --- LEAVE-ONE-SESSION-OUT ---\n",
        "for test_idx in range(len(sessions_X)):\n",
        "    print(f\"\\nðŸ“Œ Teste auf Session {test_idx+1}/{len(sessions_X)}\")\n",
        "\n",
        "    X_test = sessions_X[test_idx]\n",
        "    y_test = sessions_y[test_idx]\n",
        "\n",
        "    X_train = np.concatenate([x for i, x in enumerate(sessions_X) if i != test_idx])\n",
        "    y_train = np.concatenate([y for i, y in enumerate(sessions_y) if i != test_idx])\n",
        "\n",
        "    # Label Encoding\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(y_train)\n",
        "    y_test_enc = le.transform(y_test)\n",
        "\n",
        "    # Modell\n",
        "    model = models.Sequential([\n",
        "        layers.Conv1D(64, 3, activation='relu', input_shape=X_train.shape[1:]),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.LSTM(64),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(len(le.classes_), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training\n",
        "    history = model.fit(X_train, y_train_enc,\n",
        "                        validation_split=0.1,\n",
        "                        epochs=20,\n",
        "                        batch_size=32,\n",
        "                        verbose=0)\n",
        "\n",
        "    # Test\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test_enc, verbose=0)\n",
        "    print(f\"âœ… Test-Accuracy auf Session {test_idx+1}: {test_acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "0dFExVRd2nWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§° Erweiterter Code-Block mit Bericht\n",
        "\n",
        "FÃ¼ge diesen Teil nach der Modellbewertung in deiner for test_idx in range(...)-Schleife ein:"
      ],
      "metadata": {
        "id": "RCMuMtNe5McH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_accuracies = []\n",
        "all_reports = []\n",
        "\n",
        "for test_idx in range(len(sessions_X)):\n",
        "    print(f\"\\nðŸ“Œ Teste auf Session {test_idx+1}/{len(sessions_X)}\")\n",
        "\n",
        "    # [ ... gleicher Trainingscode wie zuvor ... ]\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test_enc, verbose=0)\n",
        "    print(f\"âœ… Test-Accuracy auf Session {test_idx+1}: {test_acc:.2f}\")\n",
        "    all_accuracies.append(test_acc)\n",
        "\n",
        "    # Klassifikationsbericht\n",
        "    y_pred = model.predict(X_test, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    report = classification_report(y_test_enc, y_pred_classes, target_names=le.classes_, output_dict=True)\n",
        "    all_reports.append(report)\n",
        "\n",
        "    # Confusion Matrix anzeigen (optional)\n",
        "    cm = confusion_matrix(y_test_enc, y_pred_classes)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
        "    disp.plot(xticks_rotation=45)\n",
        "    plt.title(f\"Confusion Matrix â€“ Session {test_idx+1}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "TQ_wkYBR5KgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§¾ Zusammenfassung nach dem Training:\n",
        "\n",
        "Nach der Schleife kannst du am Ende folgendes hinzufÃ¼gen, um einen Bericht zu erzeugen:"
      ],
      "metadata": {
        "id": "h3n3VWGx6FZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Durchschnittliche Accuracy Ã¼ber alle Sessions\n",
        "mean_acc = np.mean(all_accuracies)\n",
        "print(\"\\nðŸ“ˆ Zusammenfassung:\")\n",
        "for i, acc in enumerate(all_accuracies):\n",
        "    print(f\"  Session {i+1}: Accuracy = {acc:.2f}\")\n",
        "print(f\"\\nâœ… Durchschnittliche Test-Accuracy Ã¼ber alle Sessions: {mean_acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "KyG5h86K6BPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Š Bonus: CSV speichern (optional)"
      ],
      "metadata": {
        "id": "RNOOhWmZ6KiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "summary = {'Session': [], 'Accuracy': []}\n",
        "for i, acc in enumerate(all_accuracies):\n",
        "    summary['Session'].append(f\"Session_{i+1}\")\n",
        "    summary['Accuracy'].append(acc)\n",
        "\n",
        "df_summary = pd.DataFrame(summary)\n",
        "df_summary.to_csv(\"session_accuracy_report.csv\", index=False)\n",
        "print(\"ðŸ“ Bericht gespeichert als session_accuracy_report.csv\")\n"
      ],
      "metadata": {
        "id": "XEshR2wm6QQd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}